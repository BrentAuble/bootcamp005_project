{
    "collab_server" : "",
    "contents" : "############################################\n############################################\n#####[08] Cluster Analysis Lecture Code#####\n############################################\n############################################\n\n\n\n###########################\n#####Tools for K-Means#####\n###########################\n#Retrieving the numerical measures of the iris dataset.\niris.meas = iris[, -5]\nsummary(iris.meas)\nsapply(iris.meas, sd)\n\n#Standardizing the variables.\niris.scale = as.data.frame(scale(iris.meas))\nsummary(iris.scale)\nsapply(iris.scale, sd)\n\n#Visualizing the width measurements.\nplot(iris.scale$Petal.Width, iris.scale$Sepal.Width,\n     xlab = \"Petal Width\", ylab = \"Sepal Width\",\n     main = \"Scaled Iris Data\")\n\n#Conducting the K-Means algorithm on the whole dataset.\nset.seed(0)\nkm.iris = kmeans(iris.scale, centers = 3)\n\n#Inspecting the output of the kmeans() function.\nkm.iris\nnames(km.iris)\n#Visualizing the results against the truth.\npar(mfrow = c(1, 2))\nplot(iris.scale$Petal.Width, iris.scale$Sepal.Width,\n     xlab = \"Petal Width\", ylab = \"Sepal Width\",\n     main = \"Single K-Means Attempt\", col = km.iris$cluster)\nplot(iris.scale$Petal.Width, iris.scale$Sepal.Width,\n     xlab = \"Petal Width\", ylab = \"Sepal Width\",\n     main = \"True Species\", col = iris$Species)\n\n#Plotting the cluster centers over the data.\npar(mfrow = c(1, 1))\nplot(iris.scale$Petal.Width, iris.scale$Sepal.Width,\n     xlab = \"Petal Width\", ylab = \"Sepal Width\",\n     main = \"Single K-Means Attempt\", col = km.iris$cluster)\npoints(km.iris$centers[, 4], km.iris$centers[, 2], pch = 16, col = \"blue\")\n\n#A function to help determine the number of clusters when we do not have an\n#idea ahead of time.\nwssplot = function(data, nc = 15, seed = 0) {\n  wss = (nrow(data) - 1) * sum(apply(data, 2, var))\n  for (i in 2:nc) {\n    set.seed(seed)\n    wss[i] = sum(kmeans(data, centers = i, iter.max = 100, nstart = 100)$withinss)\n  }\n  plot(1:nc, wss, type = \"b\",\n       xlab = \"Number of Clusters\",\n       ylab = \"Within-Cluster Variance\",\n       main = \"Scree Plot for the K-Means Procedure\")\n}\n\n#Visualizing the scree plot for the scaled iris data; 3 seems like a plausible\n#choice.\nwssplot(iris.scale)\n\n#It is important to note the non-determininstic nature of the K-Means algorithm.\n#Using the Old Faithful dataset.\nfaithful.scale = scale(faithful)\nsummary(faithful.scale)\n\n#Visualizing the scaled data.\npar(mfrow = c(1, 1))\nplot(faithful.scale)\n\n#Determining the number of clusters.\nwssplot(faithful.scale)\n\n#Clearly, by both visual inspection and an analysis of the scree plot, a 2\n#cluster solution is the most appropriate; however, let's see what happens if\n#we search for a 3 cluster solution.\nset.seed(0)\nkm.faithful1 = kmeans(faithful.scale, centers = 3) #Running the K-means procedure\nkm.faithful2 = kmeans(faithful.scale, centers = 3) #5 different times, but with\nkm.faithful3 = kmeans(faithful.scale, centers = 3) #only one convergence of the\nkm.faithful4 = kmeans(faithful.scale, centers = 3) #algorithm each time.\nkm.faithful5 = kmeans(faithful.scale, centers = 3)\n\n#Running the algorithm 100 different times and recording the solution with the\n#lowest total within-cluster variance.\nset.seed(0)\nkm.faithfulsim = kmeans(faithful.scale, centers = 3, nstart = 100)\n\n#Visually & numerically inspecting the results.\npar(mfrow = c(2, 3))\nplot(faithful, col = km.faithful1$cluster,\n     main = paste(\"Single K-Means Attempt #1\\n WCV: \",\n                  round(km.faithful1$tot.withinss, 4)))\nplot(faithful, col = km.faithful2$cluster,\n     main = paste(\"Single K-Means Attempt #2\\n WCV: \",\n                  round(km.faithful2$tot.withinss, 4)))\nplot(faithful, col = km.faithful3$cluster,\n     main = paste(\"Single K-Means Attempt #3\\n WCV: \",\n                  round(km.faithful3$tot.withinss, 4)))\nplot(faithful, col = km.faithful4$cluster,\n     main = paste(\"Single K-Means Attempt #4\\n WCV: \",\n                  round(km.faithful4$tot.withinss, 4)))\nplot(faithful, col = km.faithful5$cluster,\n     main = paste(\"Single K-Means Attempt #5\\n WCV: \",\n                  round(km.faithful5$tot.withinss, 4)))\nplot(faithful, col = km.faithfulsim$cluster,\n     main = paste(\"Best K-Means Attempt out of 100\\n WCV: \",\n                  round(km.faithfulsim$tot.withinss, 4)))\n\n\n\n###########################################\n#####Tools for Hierarchical Clustering#####\n###########################################\nlibrary(flexclust) #Loading the flexclust library.\ndata(nutrient) #Loading the nutrient data.\nhelp(nutrient) #Inspecting the data set; nutrients in meat, fish, and fowel.\nnutrient\n\n#Notice that the nutrient columns are in different measurements: calories,\n#grams, and milligrams.\nsummary(nutrient)\nsapply(nutrient, sd)\n\n#We should scale the data.\nnutrient.scaled = as.data.frame(scale(nutrient))\nsummary(nutrient.scaled)\nsapply(nutrient.scaled, sd)\n\n#We need to calcualte the pairwise distances between observations.\nd = dist(nutrient.scaled)\n\n#Using the hclust() function, we define the linkage manner by which we will\n#cluster our data.\nfit.single = hclust(d, method = \"single\")\nfit.complete = hclust(d, method = \"complete\")\nfit.average = hclust(d, method = \"average\")\n\n#Creating various dendrograms.\npar(mfrow = c(1, 3))\nplot(fit.single, hang = -1, main = \"Dendrogram of Single Linkage\")\nplot(fit.complete, hang = -1, main = \"Dendrogram of Complete Linkage\")\nplot(fit.average, hang = -1, main = \"Dendrogram of Average Linkage\")\n\n#Cut the dendrogram into groups of data.\nclusters.average = cutree(fit.average, k = 5)\nclusters.average\n\n#Viewing the groups of data.\ntable(clusters.average)\n\n#Aggregating the original data by the cluster assignments.\naggregate(nutrient, by = list(cluster = clusters.average), median)\n\n#Aggregating the scaled data by the cluster assignments.\naggregate(nutrient.scaled, by = list(cluster = clusters.average), median)\n\n#Visualizing the groups in the dendrogram.\npar(mfrow = c(1, 1))\nplot(fit.average, hang = -1, main = \"Dendrogram of Average Linkage\\n5 Clusters\")\nrect.hclust(fit.average, k = 5)\n\n#-Sardines form their own cluster and are much higher in calcium than the other\n# food groups.\n#-Beef heart is also a singleton and is high in protein and iron.\n#-The clam cluster is low in protein and high in iron.\n#-The items in the cluster containing beef roast to simmered pork are high in\n# energy and fat.\n#-The largest group (from mackerel to bluefish) is relatively low in iron.",
    "created" : 1464014791599.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1921440165",
    "id" : "FF9D4F79",
    "lastKnownWriteTime" : 1464010531,
    "last_content_update" : 1464015258005,
    "path" : "C:/Users/Joseph/Desktop/NY/week7/[08] Cluster Analysis Lecture Code.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}